#+PROPERTY: header-args:clojure :tangle ../../../../../src/curbside/ml/models/xgboost.clj :mkdirp yes :noweb yes :padline yes :results silent :comments link
#+OPTIONS: toc:2

#+TITLE: XGBoost Models

* Table of Contents                                             :toc:noexport:
- [[#introduction][Introduction]]
  - [[#namespace-definition][Namespace definition]]
- [[#train][Train]]
- [[#dispose][Dispose]]
- [[#predict][Predict]]
- [[#hyperparameters-optimization][Hyperparameters optimization]]
- [[#save-and-load][Save and load]]
- [[#tests][Tests]]
  - [[#namespace-definition-1][Namespace definition]]
  - [[#training-tests][Training tests]]
  - [[#save-and-load-tests][Save and load tests]]

* Introduction

This document defines a simple Clojure interface to XGBoost, a library for gradient-boosted decision trees. This library gives extremely good performance on many problems and is becoming very popular. You can learn more about it at [[https://xgboost.readthedocs.io/en/latest/][the XGBoost website]].

** Namespace definition

#+BEGIN_SRC clojure
(ns curbside.ml.models.xgboost
  (:refer-clojure :exclude [load])
  (:require
   [clojure.java.io :as io]
   [clojure.tools.logging :as log]
   [clojure.spec.alpha :as s]
   [clojure.string :as string]
   [clojure.walk :as walk]
   [curbside.ml.training-sets.conversion :as conversion]
   [curbside.ml.training-sets.sampling :as sampling]
   [curbside.ml.utils.parsing :as parsing])
  (:import
   (ml.dmlc.xgboost4j LabeledPoint)
   (ml.dmlc.xgboost4j.java Booster DMatrix XGBoost XGBoostError)))

(def default-num-rounds 10)
#+END_SRC

* Train

To train, we just need to load the training set into a DMatrix, convert the
parameters into a Java Map, and call the =train= method on =XGBoost=.

#+BEGIN_SRC clojure
(defn- get-non-nil-values-indices
  "Get a vector of non-nil indices from a vector `v`"
  [vec]
  (->> vec
       (keep-indexed (fn [i v]
                       (when v
                         i)))
       int-array))

(defn- ->LabeledPoint
  "Create a labeled point from a vector. The vector can be dense or sparse."
  [v]
  (LabeledPoint.
   (->> v
        first
        parsing/parse-float)
   (->> v
        rest
        (map parsing/parse-float)
        get-non-nil-values-indices)
   (->> v
        rest
        (keep parsing/parse-float)
        float-array)))

(defn- filepath->DMatrix
  [filepath]
  (let [columns-keys (conversion/csv-column-keys filepath)
        vectors (->> filepath
                     (conversion/csv-to-maps)
                     (map #(->LabeledPoint ((apply juxt columns-keys) %))))]
    (DMatrix. (.iterator vectors) nil)))

(defn- set-sample-weights!
  "Sets the sample weights vector of a DMatrix. Mutates the DMatrix and returns
   nil."
  [dmatrix weights]
  (assert (= (count weights) (.rowNum dmatrix)))
  (.setWeight dmatrix (float-array weights)))

(defn- split-DMatrix
  "Split the end of a DMatrix off into a second DMatrix.
   Returns the two parts of the DMatrix.

   E.g. (split-DMatrix m 0.2) returns [m1 m2], where m2 is the last 20% of
   the dataset."
  [m split-portion]
  (let [n (.rowNum m)
        split-count (- n (Math/floor (* split-portion n)))
        first-indices (int-array (range 0 split-count))
        last-indices (int-array (range split-count n))]
    [(.slice m first-indices) (.slice m last-indices)]))

(defn train
  ([training-set-path hyperparams]
   (train training-set-path hyperparams nil))
  ([training-set-path
    {:keys [early-stopping-rounds num-rounds validation-set-size]
     :as hyperparameters}
    example-weights-path]
   (let [dm (filepath->DMatrix training-set-path)
         weights (when example-weights-path (sampling/filepath->sample-weights example-weights-path))
         _ (when weights (set-sample-weights! dm weights))
         [train val] (split-DMatrix dm (or validation-set-size 0.0))
         booster (or (:booster hyperparameters) "gbtree")
         model (XGBoost/train
                train
                (walk/stringify-keys hyperparameters)
                (int (or num-rounds default-num-rounds))
                (if validation-set-size {"validation" val} {})
                nil
                nil
                nil
                (or early-stopping-rounds 0))]
     (.setAttr model "booster" booster)
     {:xgboost-model model
      :booster booster})))
#+END_SRC

* Dispose

XGBoost is a thin wrapper around a C++ library, so most memory is allocated off-heap. We need to clean it up manually by calling =dispose=. If we do not, the GC will erroneously believe that the allocated models are very small, when they could actually be multiple megabytes in size. This causes the GC to abstain from collecting them, and the server will eventually run out of memory.

#+BEGIN_SRC clojure
(defn dispose
  "Frees the memory allocated to given model."
  [model]
  (locking model
    (.dispose (:xgboost-model model))))
#+END_SRC

* Predict

The XGBoost library uses its own matrix data type, called DMatrix. We define
some functions to convert data to/from DMatrices.

#+BEGIN_SRC clojure
(defn- get-xgboost-handle
  "Gets the internal handle field that points to the underlying C++ Booster
   object."
  [^Booster obj]
  (let [m (.. obj getClass (getDeclaredField "handle"))]
    (. m (setAccessible true))
    (. m (get obj))))

(defn- ->predict-DMatrix
  "Convert a 1D vec of floats into an DMatrix meant for use as an input to a
  Booster's .predict() method."
  [vec]
  (DMatrix. (.iterator [(->LabeledPoint vec)]) nil))

(defn predict
  [{:keys [xgboost-model booster] :as _model} hyperparameters feature-vector]
  (let [;; Pad to add a dummy label at the front of the vector.
        ;; It will be ignored when doing prediction
        dmatrix (->predict-DMatrix (into [1.0] feature-vector))]
    (->
     ;; lock for mutual exclusion w.r.t. dispose.
     (locking xgboost-model
       ;; hack: most xgboost code paths check that handle is not null and throw
       ;; an error, but sometimes calling predict just segfaults when the
       ;; handle is a null pointer.
       (if (= 0 (get-xgboost-handle xgboost-model))
         (throw (XGBoostError. "already disposed."))
         (if (= booster "dart")
           (.predict xgboost-model dmatrix false Integer/MAX_VALUE) ;; Pass a large integer to use all available trees in the model
           (.predict xgboost-model dmatrix))))
     (ffirst))))
#+END_SRC

* Hyperparameters optimization

For more details, see [[https://xgboost.readthedocs.io/en/latest/parameter.html][The docs]].

| hyper-parameter          | description                                                                             | value type | possible values                               |                    default |
|--------------------------+-----------------------------------------------------------------------------------------+------------+-----------------------------------------------+----------------------------|
| =alpha=                  | L1 regularization term.                                                                 | =decimal=  | =[0.0,...,1.0]=                               |                        0.0 |
| =base_score=             | Initial prediction score for all instances.                                             | =decimal=  | =[0.0,...]=                                   |                        0.5 |
| =booster=                | Which base model to use                                                                 | string     | =[gbtree, gblinear, dart]=                    |                     gbtree |
| =colsample_bylevel=      | Subsample ratio of columns for each split by level.                                     | =decimal=  | =[0.0,...,1.0]=                               |                        1.0 |
| =colsample_bytree=       | Subsample ratio of columns when constructing trees.                                     | =decimal=  | =[0.0,...,1.0]=                               |                        1.0 |
| =early-stopping-rounds=  | max number of rounds to boost without validation set improvements                       | =integer=  | =[0,...]=                                     |               0 (disabled) |
| =eta=                    | Step size shrinkage for updates.                                                        | =decimal=  | =[0.0,...,1.0]=                               |                        0.3 |
| =gamma=                  | Min loss reduction required to add a partition.                                         | =decimal=  | =[0.0, ...]=                                  |                          0 |
| =grow_policy=            | Controls how new nodes are added.                                                       | =string=   | =[depthwise, lossguide]=                      |                  depthwise |
| =lambda=                 | L2 regularization term.                                                                 | =decimal=  | =[0.0,...,1.0]=                               |                        1.0 |
| =max_bin=                | For hist tree_method, max number of bins.                                               | =integer=  | =[0,...]=                                     |                        256 |
| =max_delta_step=         | Max delta step for each leaf output.                                                    | =decimal=  | =[0,...]=                                     |                          0 |
| =max_depth=              | Max tree depth.                                                                         | =integer=  | =[0,...]=                                     |                          6 |
| =max_leaves=             | Max number of leaves for lossguide grow_policy                                          | =integer=  | =[0,...]=                                     |                          0 |
| =min_child_weight=       | Min sum of instance weight needed in a child node.                                      | =decimal=  | =[0,...]=                                     |                          1 |
| =normalize_type=         | Normalization algorithm for dart booster.                                               | =string=   | =[tree, forest]=                              |                       tree |
| =nthread=                | Number of parallel training threads                                                     | int        | =[1, ...]=                                    | number of cores on machine |
| =objective=              | Objective function to use.                                                              | =string=   | Many values. See official docs.               |                 reg:linear |
| =one_drop=               | Flag for dart booster: always drop at least one tree.                                   | =integer=  | =[0,1]=                                       |                          0 |
| =predictor=              | Whether to compute predictions with CPU or GPU                                          | =string=   | =[cpu_predictor, gpu_predictor]=              |              cpu_predictor |
| =process_type=           | Type of boosting process to run.                                                        | =string=   | =[default, update]=                           |                    default |
| =rate_drop=              | Dropout rate for dart booster.                                                          | =decimal=  | =[0.0,...,1.0]=                               |                        0.0 |
| =refresh_leaf=           | Param for the refresh updater plugin                                                    | =integer=  | =[0,1]=                                       |                          1 |
| =sample_type=            | Sampling algorithm for dart booster.                                                    | =string=   | =[uniform, weighted]=                         |                    uniform |
| =scale_pos_weight=       | Balance of pos/neg weights, for unbalanced data.                                        | =decimal=  | =[0.0...1.0]=                                 |                        1.0 |
| =seed=                   | Random seed.                                                                            | =integer=  | Any.                                          |                          0 |
| =silent=                 | Whether to print log messages while training                                            | int        | =[0,...,1]=                                   |                          0 |
| =sketch_eps=             | For approx tree_method.                                                                 | =decimal=  | =[0.0...1.0]=                                 |                       0.03 |
| =skip_drop=              | Probability of skipping dropout for dart booster.                                       | =decimal=  | =[0.0,...,1.0]=                               |                        0.0 |
| =subsample=              | Subsample ratio for training instances                                                  | =decimal=  | =[0.0,...,1.0]=                               |                        1.0 |
| =tree_method=            | Tree construction algorithm.                                                            | =string=   | =[auto,exact,approx,hist,gpu_exact,gpu_hist]= |                       auto |
| =tweedie_variance_power= | Param for objective=reg:tweedie                                                         | =decimal=  | =[0.0,...,1.0]=                               |                        1.5 |
| =updater=                | Comma-separated string of tree updaters.                                                | =string=   | See official docs.                            |        grow_colmaker,prune |
| =validation-set-size=    | What portion of training set to use for validation.                                     | =decimal=  | =[0.0,...,1.0]=                               |                        0.0 |
| =weight-mean=            | Mean of the Gaussian PDF for weights. Must also set weight-label-name and weight-stddev | =string=   | Any                                           |                       null |
| =weight-label-name=      | Label to compare to weight-mean to set sample weights.                                  | =string=   | Any                                           |                       null |
| =weight-stddev=          | Standard deviation of Gaussian PDF used to set sample weights.                          | =decimal=  | Any                                           |                       null |
#+TBLFM: $4=validation-set-size=: [0.0,...,1.0], =early-stopping-rounds=: any integer

#+BEGIN_SRC clojure
(defn iff
  [& args]
  (or (every? identity args)
      (every? not args)))

(s/def ::double-between-zero-and-one (s/double-in :min 0.0 :max 1.0))
(s/def ::positive-double (s/double-in :min 0.0 :infinite? false))

(s/def ::booster #{"gbtree" "gblinear" "dart"})
(s/def ::silent (s/int-in 0 2))
(s/def ::nthread integer?)
(s/def ::learning_rate ::double-between-zero-and-one)
(s/def ::gamma ::positive-double)
(s/def ::max_delta_step ::positive-double)
(s/def ::max_depth integer?)
(s/def ::min_child_weight ::double-between-zero-and-one)
(s/def ::subsample ::double-between-zero-and-one)
(s/def ::colsample_bytree ::double-between-zero-and-one)
(s/def ::colsample_bylevel ::double-between-zero-and-one)
(s/def ::lambda ::double-between-zero-and-one)
(s/def ::alpha ::double-between-zero-and-one)
(s/def ::tree_method #{"auto" "exact" "approx" "hist" "gpu_exact" "gpu_hist"})
(s/def ::sketch_eps ::double-between-zero-and-one)
(s/def ::scale_pos_weight ::double-between-zero-and-one)
(s/def ::updater
  #{"grow_colmaker"
    "distcol"
    "grow_histmaker"
    "grow_local_histmaker"
    "grow_skmaker"
    "sync"
    "refresh"
    "prune"})
(s/def ::refresh_leaf (s/int-in 0 2))
(s/def ::process_type #{"default" "update"})
(s/def ::grow_policy #{"depthwise" "lossguide"})
(s/def ::max_leaves integer?)
(s/def ::max_bin integer?)
(s/def ::predictor #{"cpu_predictor" "gpu_predictor"})
(s/def ::sample_type #{"uniform" "weighted"})
(s/def ::normalize_type #{"tree" "forest"})
(s/def ::rate_drop ::double-between-zero-and-one)
(s/def ::one_drop (s/int-in 0 2))
(s/def ::skip_drop ::double-between-zero-and-one)
(s/def ::updater #{"shotgun" "coord_descent"})
(s/def ::tweedie_variance_power ::double-between-zero-and-one)
(s/def ::objective #{"reg:logistic"
                     "binary:logistic"
                     "binary:logitraw"
                     "binary:hinge"
                     "gpu:reg:linear"
                     "gpu:reg:logistic"
                     "gpu:binary:logistic"
                     "gpu:binary:logitraw"
                     "count:poisson"
                     "survival:cox"
                     "multi:softmax"
                     "multi:softprob"
                     "rank:pairwise"
                     "reg:gamma"
                     "reg:tweedie"
                     "reg:squarederror"
                     "reg:squaredlogerror"})
(s/def ::base_score (s/double-in :infinite? false :NaN? false))
(s/def ::seed integer?)
(s/def ::num-rounds integer?)
(s/def ::validation-set-size ::double-between-zero-and-one)
(s/def ::early-stopping-rounds integer?)
(s/def ::weight-mean (s/double-in :infinite? false :NaN? false))
(s/def ::weight-label-name (s/or :kw keyword? :str string?))
(s/def ::weight-stddev (s/double-in :infinite? false :NaN? false))

(s/def ::hyperparameters
  (s/and
   (s/keys :req-un [::num-rounds]
           :opt-un [::booster
                    ::silent
                    ::nthread
                    ::learning_rate
                    ::gamma
                    ::max_depth
                    ::min_child_weight
                    ::max_delta_step
                    ::subsample
                    ::colsample_bytree
                    ::colsample_bylevel
                    ::lambda
                    ::alpha
                    ::tree_method
                    ::sketch_eps
                    ::scale_pos_weight
                    ::updater
                    ::refresh_leaf
                    ::process_type
                    ::grow_policy
                    ::max_leaves
                    ::max_bin
                    ::predictor
                    ::sample_type
                    ::normalize_type
                    ::rate_drop
                    ::one_drop
                    ::skip_drop
                    ::updater
                    ::tweedie_variance_power
                    ::objective
                    ::base_score
                    ::seed
                    ::validation-set-size
                    ::early-stopping-rounds
                    ::weight-mean
                    ::weight-label-name
                    ::weight-stddev])
   #(iff (:validation-set-size %) (:early-stopping-rounds %))
   #(iff (:weight-mean %) (:weight-label-name %) (:weight-stddev %))))
#+END_SRC

* Save and load

The standard =save-model= and =load-model= functions can be defined easily
using standard XGBoost methods.

#+NAME: model management
#+BEGIN_SRC clojure
(defn save
  [{:keys [xgboost-model] :as _model} filepath]
  (.saveModel xgboost-model filepath)
  [filepath])

(defn- get-booster-from-attributes
  [xgboost-model]
  (.getAttr xgboost-model "booster"))

(defn- ^:deprecated get-booster-from-file
  [filepath]
  (log/warn "[xgboost] DEPRECATED Loading the booster type from file")
  (let [first-binary-line (first (line-seq (io/reader filepath)))]
    (cond
      (string/includes? first-binary-line "gbtree")
      "gbtree"

      (string/includes? first-binary-line "dart")
      "dart"

      :else
      (do
        (log/error "[xgboost] Could not find the booster type in the file. Assuming gbtree. This can lead to undefined behaviors if it was a dart booster.")
        "gbtree"))))

(defn load
  [filepath]
  (let [m (XGBoost/loadModel ^String filepath)]
    {:xgboost-model m
     :booster (or (get-booster-from-attributes m)
                  (get-booster-from-file filepath))}))
#+END_SRC

* Tests

** Namespace definition

#+NAME: test-namespace
#+BEGIN_SRC clojure :tangle ../../../../../test/curbside/ml/models/xgboost_test.clj
(ns curbside.ml.models.xgboost-test
  (:require
   [clojure.core.async :refer [alts!! timeout thread-call]]
   [clojure.test :refer [deftest is testing]]
   [curbside.ml.models.xgboost :as xgboost]
   [curbside.ml.training-sets.conversion :as conversion]
   [curbside.ml.utils.tests :as tutils]
   [clojure.java.io :as io])
  (:import
   [java.util Arrays]
   [ml.dmlc.xgboost4j.java Booster]))
#+END_SRC

** Training tests

Here, the goal of the test is simply to see if we can train an xgboost model on a dataset and then do a prediction. To do so, we train on a dummy dataset where all the labels are the same.

#+BEGIN_SRC clojure :tangle ../../../../../test/curbside/ml/models/xgboost_test.clj
(deftest test-split-dmatrix
  (let [dm (#'xgboost/filepath->DMatrix tutils/dummy-regression-single-label-training-set-path)
        [dm1 dm2] (#'xgboost/split-DMatrix dm 0.2)]
    (is (= 9 (.rowNum dm1)))
    (is (= 2 (.rowNum dm2)))))

(deftest test-train-and-predict
  (testing "given a dataset with a single label, when training, then the model always return a prediction close to this label."
    (let [hyperparameters {:verbosity 3 :num-rounds 5 :booster "dart" :learning_rate 0.9 :objective "reg:squarederror"}
          model (xgboost/train tutils/dummy-regression-single-label-training-set-path hyperparameters)
          prediction (xgboost/predict model hyperparameters [0 0])]
      (is (tutils/approx= 0.0 prediction 1e-1)))))

(deftest test-sample-weighting
  (testing "given a dataset with a single label, when training with sample weighting, then the model always return a prediction close to this label."
    (let [hyperparameters {:verbosity 3 :num-rounds 5 :booster "dart"
                           :learning_rate 0.9 :objective "reg:squarederror"
                           :weight-mean 0.5 :weight-label-name "label"
                           :weight-stddev 1.0}
          model (xgboost/train
                 tutils/dummy-regression-single-label-training-set-path
                 hyperparameters
                 tutils/dummy-example-weights-path)
          prediction (xgboost/predict model hyperparameters [0 0])]
      (is (tutils/approx= 0.0 prediction 1e-1)))))

(deftest test-early-stopping
  (testing "early stopping stops early"
    (let [hyperparameters
          {:num-rounds 999999 :booster "dart"
           :validation-set-size 0.5
           :early-stopping-rounds 5}
          timeout-ch (timeout 2000)
          model-ch (thread-call
                    #(xgboost/train
                      tutils/dummy-regression-single-label-training-set-path
                      hyperparameters))
          [v c] (alts!! [timeout-ch model-ch])]
      (is (= c model-ch))
      (is (= Booster (type (:xgboost-model v)))))))
#+END_SRC

** Save and load tests

#+BEGIN_SRC clojure :tangle ../../../../../test/curbside/ml/models/xgboost_test.clj
(deftest test-save-and-load-model
  (testing "given a trained model, when saving and loading, then the loaded model is the model that was saved."
    (let [hyperparameters {:booster "gbtree"}
          model (xgboost/train tutils/dummy-regression-single-label-training-set-path hyperparameters)
          model-path (tutils/create-temp-path ".xgb")]
      (xgboost/save model model-path)
      (let [loaded-model (xgboost/load model-path)]
        (is (= (dissoc model :xgboost-model)
               (dissoc model :xgboost-model)))
        (is (Arrays/equals (.toByteArray (:xgboost-model model))
                           (.toByteArray (:xgboost-model loaded-model))))))))

(deftest test-load-legacy-model
  (testing "given a legacy dart model, when loading, then the booster is dart"
    (is (= "dart" (:booster (xgboost/load (tutils/resource-name-to-path-str "models/dart-legacy.xgb"))))))
  (testing "given a legacy gbtree model, when loading, then the booster is gbtree"
    (is (= "gbtree" (:booster (xgboost/load (tutils/resource-name-to-path-str "models/gbtree-legacy.xgb")))))))
#+END_SRC
