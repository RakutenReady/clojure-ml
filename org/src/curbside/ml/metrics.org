#+PROPERTY: header-args:clojure :tangle ../../../../src/curbside/ml/metrics.clj :mkdirp yes :noweb yes :padline yes :results silent :comments link
#+OPTIONS: toc:2

#+TITLE: Metrics

* Table of Contents                                            :toc:noexport:
- [[#introduction][Introduction]]
- [[#namespace-definition][Namespace definition]]
- [[#model-metrics][Model metrics]]
- [[#metrics-optimization][Metrics Optimization]]
- [[#feature-metrics][Feature metrics]]
  - [[#specification][Specification]]
  - [[#compute-metrics][Compute metrics]]

* Introduction

The main metrics involved in =classification= tasks are:

  1. =True Positives=
  2. =False Positives=
  3. =True Negatives=
  4. =False Negatives=
  5. =Precision=
  6. =Accuracy=
  7. =Recall=
  8. =False Positive Rate=
  9. =True Positive Rate=
  10. =Area Under The Curve=
  11. =Area Under PRC=
  12. =F-Measures=
      1. =F1=

The main metrics involved in =regression= tasks are:

  1. =Correlation Coefficient=
  2. =Error Rate=
  3. =Root Relative Square Error=
  4. =Root Mean Square Error=
  5. =Root Mean Prior Squared Error=
  6. =Relative Absolute Error=

* Namespace definition

#+BEGIN_SRC clojure
(ns curbside.ml.metrics
  (:refer-clojure :exclude [comparator])
  (:require
   [clojure.spec.alpha :as s]
   [curbside.ml.training-sets.conversion :as conversion]
   [curbside.ml.utils.spec :as spec-utils]
   [curbside.ml.utils.stats :as stats]
   [curbside.ml.utils.parsing :as parsing]
   [curbside.ml.utils.weka :as weka])
  (:import
    (weka.classifiers.evaluation ConfusionMatrix ThresholdCurve)
    (weka.attributeSelection AttributeSelection
                             CfsSubsetEval
                             CorrelationAttributeEval
                             GainRatioAttributeEval
                             InfoGainAttributeEval
                             ReliefFAttributeEval
                             SymmetricalUncertAttributeEval
                             OneRAttributeEval
                             GreedyStepwise
                             Ranker)
    (java.util ArrayList)))
#+END_SRC

* Model metrics

#+BEGIN_SRC clojure
(defn model-metrics
  "Calculate all the metrics given a map containing vector of predictions, abs-error
  and square-error. Return a map of the computed metrics."
  [predictor-type eval-metrics]
  (let [{:keys [predictions abs-error square-error n]} eval-metrics
        predictions (if (every? nil? predictions) (ArrayList. []) (ArrayList. predictions))
        confusion-matrix (ConfusionMatrix. (into-array String ["1.0" "0.0"]))
        _ (.addPredictions confusion-matrix predictions)
        two-classes-stats (.getTwoClassStats confusion-matrix 1)
        threshold-curve (ThresholdCurve.)
        instances (.getCurve threshold-curve predictions)]
    (merge {:mean-absolute-error (stats/mean-absolute-error n abs-error)
            :root-mean-square-error (stats/root-mean-square-error n square-error)
            :total-number-instances (double n)}
           (when (= predictor-type :classification)
             {:tp (parsing/nan->nil (.getTruePositive two-classes-stats))
              :fp (parsing/nan->nil (.getFalsePositive two-classes-stats))
              :tn (parsing/nan->nil (.getTrueNegative two-classes-stats))
              :fn (parsing/nan->nil (.getFalseNegative two-classes-stats))
              :recall (parsing/nan->nil (.getRecall two-classes-stats))
              :precision (parsing/nan->nil (.getPrecision two-classes-stats))
              :fpr (parsing/nan->nil (.getFalsePositiveRate two-classes-stats))
              :tpr (parsing/nan->nil (.getTruePositiveRate two-classes-stats))
              :accuracy (parsing/nan->nil
                         (/ (+ (.getTruePositive two-classes-stats) (.getTrueNegative two-classes-stats))
                            (+ (.getTruePositive two-classes-stats) (.getTrueNegative two-classes-stats)
                               (.getTrueNegative two-classes-stats) (.getFalseNegative two-classes-stats))))
              :f1 (parsing/nan->nil (.getFMeasure two-classes-stats))
              :roc-auc (parsing/nan->nil (ThresholdCurve/getROCArea instances))
              :auprc (parsing/nan->nil (ThresholdCurve/getPRCArea instances))
              :kappa (parsing/nan->nil (stats/kappa confusion-matrix))
              :incorrectly-classified-instances (parsing/nan->nil (stats/incorrectly-classified confusion-matrix))

              :correctly-classified-instances (parsing/nan->nil (stats/correctly-classified confusion-matrix))
              :correctly-classified-instances-percent (parsing/nan->nil (stats/correctly-classified-percent confusion-matrix))}))))
#+END_SRC

* Metrics Optimization

Each of the classification or regression algorithm have different metrics that helps us to determine if a model if better than another a given prediction problem.

When come the time to optimize a model, we have to optimize it according to the result of one of those metrics. Some of the metrics are better when they are higher, others are better when they are lower.

What we do here is to define each of those classification and regression metrics and specify how they should be optimized: by keeping the lower and higher results.

#+NAME: metrics optimize max
#+BEGIN_SRC clojure
(defn comparator
  "Returns the comparator to use to compare a metrics' results to optimize its
  value. Returns `nil` if the metric is unknown."
  [metric]
  (get {:tp >
        :fp <
        :tn >
        :fn <
        :recall >
        :precision >
        :fpr <
        :tpr >
        :f1 >
        :roc-auc >
        :auprc >
        :kappa >
        :correlation-coefficient >
        :error-rate <
        :root-relative-square-error <
        :root-mean-square-error <
        :root-mean-prior-squared-error <
        :relative-absolute-error <
        :mean-absolute-error <}
       metric))
#+END_SRC
* Feature metrics
** Specification

| key              | type                  | description                                                                                                                                                                                                   |
|------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| =:evaluators= | =vector of evaluator= | vector of evaluators that we want to use to evaluate the features. Available evaluators are: =:cfs-subset=, =:correlation=, =:gain-ratio=, =:info-gain=, =:one-r=, =:symmetrical-uncertainty= and =:relief-f= |

Each of the evaluator are different method to evaluate features within a model. Here is a description of each of those methods:

| evaluator                  | description                                                                                                                                                                                                                                                                                     |
|----------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| =:cfs-subset=              | Evaluates the worth of a subset of attributes by considering the individual predictive ability of each feature along with the degree of redundancy between them. Subsets of features that are highly correlated with the class while having low intercorrelation are preferred.                 |
| =:correlation=             | Evaluates the worth of an attribute by measuring the correlation (Pearson's) between it and the class. Nominal attributes are considered on a value by value basis by treating each value as an indicator. An overall correlation for a nominal attribute is arrived at via a weighted average. |
| =:gain-ratio=              | Evaluates the worth of an attribute by measuring the gain ratio with respect to the class.                                                                                                                                                                                                      |
| =:info-gain=               | Evaluates the worth of an attribute by measuring the information gain with respect to the class.                                                                                                                                                                                                |
| =:relief-f=                | Evaluates the worth of an attribute by repeatedly sampling an instance and considering the value of the given attribute for the nearest instance of the same and different class. Can operate on both discrete and continuous class data.                                                       |
| =:one-r=                   | Evaluates the worth of an attribute by using the OneR classifier.                                                                                                                                                                                                                               |
| =:symmetrical-uncertainty= | Evaluates the worth of an attribute by measuring the symmetrical uncertainty with respect to the class.                                                                                                                                                                                         |

#+BEGIN_SRC clojure
(s/def ::evaluator #{:cfs-subset
                     :correlation
                     :gain-ratio
                     :info-gain
                     :relief-f
                     :one-r
                     :symmetrical-uncertainty})
(s/def ::evaluators (s/coll-of ::evaluator :distinct true))
#+END_SRC

** Compute metrics

#+BEGIN_SRC clojure
(defn- get-attribute-key
  [id instances]
  (keyword
   (.name
    (.attribute instances id))))

(defn- evaluate-feature
  [evaluator instances]
  (let [attribute-selection (AttributeSelection.)
        eval (case evaluator
               :cfs-subset (CfsSubsetEval.)
               :correlation (CorrelationAttributeEval.)
               :gain-ratio (GainRatioAttributeEval.)
               :info-gain (InfoGainAttributeEval.)
               :relief-f (ReliefFAttributeEval.)
               :one-r (OneRAttributeEval.)
               :symmetrical-uncertainty (SymmetricalUncertAttributeEval.))
        search (case evaluator
                 :cfs-subset (GreedyStepwise.)
                 :correlation (Ranker.)
                 :gain-ratio (Ranker.)
                 :info-gain (Ranker.)
                 :relief-f (Ranker.)
                 :one-r (Ranker.)
                 :symmetrical-uncertainty (Ranker.))]
    (.setEvaluator attribute-selection eval)
    (.setSearch attribute-selection search)
    (.SelectAttributes attribute-selection instances)
    (if (= :cfs-subset evaluator)
      (->> (.selectedAttributes attribute-selection)
           (mapv (fn [id]
                   (get-attribute-key (int id) instances)))
           (remove #{:label}))
      (->> (.rankedAttributes attribute-selection)
           (map (fn [[id rank]]
                  {(get-attribute-key (int id) instances) rank}))
           (apply merge)))))

(defn- get-training-instances
  [training-set-csv-path predictor-type]
  (weka/problem
   (conversion/csv-to-arff training-set-csv-path predictor-type)))

(defn feature-metrics
  [training-set-csv-path predictor-type evaluators]
  {:pre [(spec-utils/check ::evaluators evaluators)]}
  (let [instances (get-training-instances training-set-csv-path predictor-type)]
    (reduce (fn [metrics evaluator]
              (assoc metrics evaluator (evaluate-feature evaluator instances)))
            {}
            evaluators)))
#+END_SRC
